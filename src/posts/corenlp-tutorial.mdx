---
title: "CoreNLP: Un tutorial"
description: "Extrayendo entidades y relaciones de ciertas paginas web"
date: "2019-08-07"
author: diana
tags: ["coreNLP", "extractor", "NLP", "tutorial"]
image: /images/corenlp.png
---

El procesamiento de lenguaje natural (NLP), uno de las 치reas de Inteligencia Artificial (AI), tiene un gran crecimiento en en el desarrollo de aplicaciones: las vemos en Bots, en los asistentes personales (como Siri). Una de las aplicaciones muy utiles es el procesamiento de informaci칩n, obtener **"insights"**. Este post trata de explicar como usar NLP para extraer informaci칩n de periodicos, imaginate desarrollar una aplicaci칩n que lea noticias de diarios como Washington Post o el New York Times y de ello poder sacar "insights". Sin embargo, en vez de ello la usaremos para analizar noticias de Per칰, un pa칤s con una muy buena comida pero con muchos problemas politicos (sus ultimos 3 presidentes procesados judicialmente 游땍). Imaginate analizar las redes de politica de tu pa칤s, pues bien, eso es lo que haremos con Per칰!

Bueno no tanto, eso tomar칤a mucho tiempo. En este tutorial, solo analizaremos los textos de ciertas paginas web sobre pol칤tica (El comercio), extraeremos las entidades (por ejemplo nombre de personas, lugares, organizaciones) y con ello mostraremos las relaciones entre ellas, formando una red! 游땏. Tomaremos un enfoque totalmente pr치ctico y usaremos muchas librer칤as en diferentes lenguajes de programaci칩n.

En s칤 el proceso que haremos es el siguiente:

1. Obtener los textos a procesar
2. Reconocer las entidades y relaciones en ella
3. Mostrar un grafo (de relaciones)

Comenzemos por el paso 1:

### 1.- Obtener los textos a procesar

Primero lo primero, identificar de donde sacar las noticias, como dijimos la sacaremos de diarios de Per칰, por ejemplo:

-   Links de noticias que se muestran en: https://elcomercio.pe/politica
    -   Url 1
    -   Url 2
    -   ...

Bueno ahora **쮺omo obtendremos el texto de esas noticias?**

Para esto, el concepto de [Web Scraping](https://es.wikipedia.org/wiki/Web_scraping) nos sera de ayuda, b치sicamente es hacer una petici칩n Http a estos sitios web, obtener el html y mediante librerias de analisis de documentos HTML (por ejemplo, [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) si usas Python), obtener el texto.

Felizmente existe una libreria en python que incluye todo esto, [Newspaper3k: Article scraping & curation](https://newspaper.readthedocs.io/en/latest/) 游녨

Lo 칰nico que debemos hacer es descargar la librer칤a ejecutando en nuestra terminal `pip3 install newspaper3` y crear un archivo (por ejemplo, `news.py`) que contenga algo como:

```python
from newspaper import Article

def extract_text(url):
    article = Article(url)
    article.download()
    article.parse()
    return article.text

urls = ['url 1','url2',...]
texts = list(map( extract_text, urls ))
```

Con lo siguiente descargariamos los textos de una lista de `urls` (nuestras noticias), con tan solo muy pocas lineas de c칩digo!

Con ello, hemos realizado la primera tarea, vayamos al siguiente paso.

### 2.- Reconocer las entidades y relaciones en el texto

El texto de por si no es 칰til si es que no es interpretado, datos no es lo mismo que informaci칩n. Es aqu칤 donde usaremos NLP para interpretar el texto, y de ello obtener las entidades (como las personas o lugares que aparecen en las noticias) y tambi칠n que relaciones a las entidades.

Pero la pregunta ahora es **쮺omo obtendremos las entidades y relaciones en los textos?** Aqui viene [CoreNLP](https://stanfordnlp.github.io/CoreNLP/) al rescate.

CoreNLP, es una librer칤a desarrollada por la Universidad de Standford en Java, que integra muchas herramientas de Procesamiento de Lenguaje Natural, entre ellas, nos ser치n de utilidad: Named Entity Recognizer (NER), que se encarga de reconocer las entidades, y el Relation Extractor Annotator, encargado de reconocer las relaciones.

Una de las caracter칤sticas muy potentes de CoreNLP es su capacidad ser usada como un servidor, es decir podemos hacerle peticiones HTTP. Por ejemplo solicitarle las entidades de un texto (GET entities?text='.......'), de esta forma podr칤amos integrarla con cualquier lenguaje de programaci칩n.

Nosotros usaremos CoreNLP de esa forma.

As칤 que primero levantemos CoreNLP, primero lo primero, dado que CoreNLP est치 desarrollado en Java, lo primero es [descargarlo](https://openjdk.java.net/install/index.html) al menos en su versi칩n 8, puedes verificarlo asi: `java -version`

Luego descargar en s칤 CoreNLP, esto requiere que descarguemos: [corenlp .jar](https://nlp.stanford.edu/software/stanford-corenlp-full-2018-10-05.zip), y dado que analizaremos noticias en espa침ol, necesitamos descargar el [.jar que contiene el modelo entrenado para el lenguaje espa침ol ](https://nlp.stanford.edu/software/stanford-spanish-corenlp-2018-10-05-models.jar);
luego pondremos estos jar en una carpeta llamada **corenlp** (o el nombre que prefieras).

Luego de esto, nos ubicaremos en la carpeta **corenlp**, y ejecutaremos lo
siguiente:

```shell
java -cp "\*" -Xmx4g edu.stanford.nlp.pipeline.StanfordCoreNLPServer \
 -serverProperties StanfordCoreNLP-spanish.properties \
 -timeout 30000 -quiet \
 -preload "tokenize,ssplit,pos,lemma,ner,depparse,coref,kbp" \
 -annotators "ner,kbp" \
 -maxCharLength 1000000 -port 9000
```

Esto levanta el servicio de CoreNLP en el puerto 9000, con una serie de opciones, por ejemplo el flag `-preload`, indica que debemos tokenizar ('tokenize' - Tokeniza el texto a procesar y devuelve la lista de 'tokens' en el Response), puedes ver m치s acerca de estas opciones [en su documentaci칩n.]

Una vez ejecutado este comando, podremos ver en localhost:9000, un contenido igual a [este link](https://corenlp.run/).

Ahora podemos hacer peticiones web para obtener las entidades y relaciones, y dado que usamos python usaremos la librer칤a **pycorenlp** para que CoreNLP lea los textos `texts` que hemos descargado en el paso anterior:

```python
# c칩digo del paso 1.
# ...
texts = list(map( extract_text, urls ))

from pycorenlp import StanfordCoreNLP
nlp = StanfordCoreNLP('http://localhost:9000')

def recognizer(text):
    return nlp.annotate(text,
                properties={
                    'annotators': 'tokenize,ssplit,pos,ner,depparse,openie,kbp',
                    'outputFormat': 'json',
                    'timeout': 90000,
                })
response = list(map(recognizer, texts))
```

En la solicitud web (request) se le especifica una lista de [annotators](https://stanfordnlp.github.io/CoreNLP/annotators.html), que son tipos de analisis que se le aplicara al texto a procesar: tokenize,ssplit,pos,ner,depparse,openie,kbp.

La respuesta a la peticion web (`response`) veremos la propiedad **sentences**, el cual contiene un arreglo de oraciones del texto a procesar, cada oracion contiene estas propiedades:
"tokens", "index", "basicDependencies", "enhancedDependencies", "enhancedPlusPlusDependencies","entitymentions","openie", "kbp".

Para este caso solo usaremos **entitymentions** (que nos da a conocer las entidades) y **openie** (que nos da a conocer las relaciones).

Por ejemplo, haciendole peticion a https://corenlp.run/, con el texto: `Donald Trump is president USA` obtendremos:

```json
{
    "docDate": "2019-08-18T22:43:06",
    "sentences": [
        {
            "index": 0,
            "openie": [
                {
                    "subject": "Donald Trump",
                    "subjectSpan": [0, 2],
                    "relation": "is",
                    "relationSpan": [2, 3],
                    "object": "president USA",
                    "objectSpan": [3, 5]
                }
            ],
            "entitymentions": [
                {
                    "docTokenBegin": 0,
                    "docTokenEnd": 2,
                    "tokenBegin": 0,
                    "tokenEnd": 2,
                    "text": "Donald Trump",
                    "characterOffsetBegin": 0,
                    "characterOffsetEnd": 12,
                    "ner": "PERSON"
                },
                {
                    "docTokenBegin": 3,
                    "docTokenEnd": 4,
                    "tokenBegin": 3,
                    "tokenEnd": 4,
                    "text": "president",
                    "characterOffsetBegin": 16,
                    "characterOffsetEnd": 25,
                    "ner": "TITLE"
                },
                {
                    "docTokenBegin": 4,
                    "docTokenEnd": 5,
                    "tokenBegin": 4,
                    "tokenEnd": 5,
                    "text": "USA",
                    "characterOffsetBegin": 26,
                    "characterOffsetEnd": 29,
                    "ner": "COUNTRY"
                }
            ]
        }
    ]
}
```

El texto de ejemplo solo contiene una oracion, pero en casos reales, el texto contendra muchas oraciones, por lo que la propiedad `sentences` sera un arreglo con muchos elementos.
Bueno ya obtuvimos las entidades y relaciones, ahora nos toca el ultimo paso, mostrar el grafo.

### Mostrar el grafo

Mostramos un ejemplo del resultado de una oracion, sabemos que por texto CoreNLP retorna muchas oraciones, bueno el resultado final tomara este formato:

```json
{ "data": {
    "nodes": {
          "0": [
              {
                  "name": "PNP",
                  "type": "ORGANIZATION",
                  "color": "#527f1c"
              },
              {
                  "name": "Luis Muguruza Delgado",
                  "type": "PERSON",
                  "color": "#04dbf4
              } ...
            ]
          "1": [ ... ],
          "2": [ ... ],
          ...
          }
    "edges": {
            "0": [
                {
                    "from": "efectivos",
                    "to": "armas",
                    "label": "empleando"
                },..
              ]
            "1": [ ...],
            "2": [ ...],
            }
}
```

Dado este formato se visualizara el grafo usando **jsnetworkx.js**

```javascript
function draw(nodes, edges) {
    var G = new jsnx.DiGraph();
    nodes_graph = nodes.map(node => [
        node["name"],
        { color: node["color"], size: 30 }
    ]);
    edges_graph = edges.map(node => [node["from"], node["to"]]);
    G.addNodesFrom(nodes_graph);
    G.addEdgesFrom(edges_graph);

    jsnx.draw(G, {
        element: "#canvas",
        withLabels: true,
        nodeAttr: {
            r: function(d) {
                return d.data.size;
            }
        },
        nodeStyle: {
            fill: function(d) {
                return d.data.color.replace("0", "5");
            }
        },
        node_style: {
            fill: "#999",
            cursor: "pointer",
            "stroke-opacity": "0",
            stroke: "white"
        },
        labelStyle: { fill: "black", "font-weigth": "bold" },
        stickyDrag: true
    });
}
```

Basandonos en la variable data mostrado anteriormente en formato json se mostrara el grafo:

```javascript
index = 0; // el primer texto de data
draw(data["nodes"][index], data["edges"][index]);
```

Luego de todo este trabajo, obtendremos un resultado similar a la siguiente image.

<figure>
    <img src="/images/corenlp.png" style="width:100%;" />
    <figcaption>
        <a href="https://extractor.qpdiam.now.sh">Link de demo</a> Grafo de
        entidades y sus relaciones
    </figcaption>
</figure>

Listo terminamos 游땏 游땏
